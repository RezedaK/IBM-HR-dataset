{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case-study: Data Science\n",
    "\n",
    "**Author**: Jacopo Ventura\n",
    "\n",
    "**Date**: 24th September 2017\n",
    "\n",
    "Dataset: HR employee attrition and performance.\n",
    "\n",
    "Tasks:\n",
    "1. Import, clean and visualize the data\n",
    "2. Bulid a predictive model of Employee churn\n",
    "3. Generate and validate hypothesis of why Employees churn \n",
    "4. Build a lookalike model of the users and reason about their groupings\n",
    "\n",
    "The goal of the project is to predict attrition. In HR termonology, attrition occurs when an employee retires or when the company eliminates his job.\n",
    "\n",
    "## 2. Bulid a predictive model of Employee churn (Attrition)\n",
    "\n",
    "We now build a predictive model of Employee churn (Attrition). We proceed as follow:\n",
    "\n",
    "- clean the dataset according to the data wrangling phase\n",
    "- generate training and test data from the dataset\n",
    "- prepare data for training the Machine Learning model\n",
    "- train different binary classifier models: logistic regression, random forest, SVM\n",
    "- fine tuning of the best model\n",
    "- test on the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages for data analysis\n",
    "import os    \n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# step 1: Import data as Pandas dataframe\n",
    "data_path='C:/Users/jacopo/Desktop/WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "dataset = pd.read_csv(data_path)   # dataset as pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset by eliminating over 18, EmployeeCount, EmployeeNumber and StandardHours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop attributes from the dataframe (see data wrangling phase)\n",
    "dataset.drop([\"EmployeeCount\",\"Over18\",\"EmployeeNumber\",\"StandardHours\"],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training and test dataset\n",
    "\n",
    "Apply stratified sampling to split dataset in test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # n_splits: number of slitted dataset; \n",
    "for train_index, test_index in split.split(dataset, dataset[\"Attrition\"]):    \n",
    "    dataset_train = dataset.loc[train_index]\n",
    "    dataset_test = dataset.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and label\n",
    "dataset_test_Features = dataset_test.drop([\"Attrition\"], axis=1) \n",
    "dataset_test_Label = dataset_test[\"Attrition\"].copy()\n",
    "\n",
    "dataset_train_Features = dataset_train.drop([\"Attrition\"], axis=1) \n",
    "dataset_train_Label = dataset_train[\"Attrition\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training the Machine Learning model\n",
    "\n",
    "In this phase, we normalize the numerical features and convert categorical variable into dummy/indicator variables. In fact, Machine Learning models works with numerical values only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define numerical attributes\n",
    "num_attribs = [\"Age\",\"DailyRate\",\"DistanceFromHome\",\"HourlyRate\",\"MonthlyIncome\",\n",
    "              \"MonthlyRate\",\"NumCompaniesWorked\",\"PercentSalaryHike\",\n",
    "              \"TotalWorkingYears\",\"TrainingTimesLastYear\",\"YearsAtCompany\",\"YearsInCurrentRole\",\n",
    "              \"YearsSinceLastPromotion\",\"YearsWithCurrManager\",\n",
    "              \"Education\",\"EnvironmentSatisfaction\",\"JobInvolvement\",\"JobLevel\",\"JobSatisfaction\",\"PerformanceRating\",\n",
    "              \"RelationshipSatisfaction\",\"StockOptionLevel\",\"WorkLifeBalance\"] \n",
    "\n",
    "# Define categorical attributes\n",
    "cat_attribs = [\"BusinessTravel\",\"Department\",\"EducationField\",\"Gender\",\"JobRole\",\n",
    "              \"MaritalStatus\",\"OverTime\"]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalize numerical features\n",
    "normalizer = preprocessing.Normalizer()\n",
    "dataset_train_FeaturesX_normalized = normalizer.fit_transform(dataset_train_Features[num_attribs])\n",
    "\n",
    "# Encoding of categorical attributes and return numpy array\n",
    "dataset_train_FeaturesX_encoded = pd.get_dummies(dataset_train_Features[cat_attribs]).as_matrix()\n",
    "\n",
    "# Merge the two feature arrays\n",
    "dataset_train_FeaturesX_encoded_normalized = np.concatenate((dataset_train_FeaturesX_normalized, \n",
    "                                                             dataset_train_FeaturesX_encoded), \n",
    "                                                             axis=1)\n",
    "\n",
    "# Convert label from categorical to numerical\n",
    "dataset_train_Label.replace(['No','Yes'],[0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Machine Learning model\n",
    "\n",
    "We train three different Machine Learning models:\n",
    "- Logistic regression\n",
    "- Random Forest\n",
    "- Support Vector Machine\n",
    "\n",
    "We evaluate the performance of each trained model through the F1_score: \n",
    "\n",
    "$F_1 = 2\\dfrac{precision * recall}{precision + recall}$\n",
    "\n",
    "using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: try some ML models\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23727456762239374"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# cross validation on the training dataset\n",
    "scores = cross_val_score(log_reg,\n",
    "                         dataset_train_FeaturesX_encoded_normalized, \n",
    "                         dataset_train_Label, \n",
    "                         cv=10, \n",
    "                         scoring='f1')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40255257828749491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=20,\n",
    "                                 max_leaf_nodes=20,\n",
    "                                 n_jobs=-1,\n",
    "                                 class_weight = 'balanced')\n",
    "\n",
    "\n",
    "# cross validation on the training dataset\n",
    "scores = cross_val_score(rnd_clf, \n",
    "                         dataset_train_FeaturesX_encoded_normalized, \n",
    "                         dataset_train_Label, \n",
    "                         cv=10, \n",
    "                         scoring='f1')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", \n",
    "              degree=3, \n",
    "              coef0=1, \n",
    "              C=5)\n",
    "\n",
    "# cross validation on the training dataset\n",
    "scores = cross_val_score(svm_clf, \n",
    "                         dataset_train_FeaturesX_encoded_normalized, \n",
    "                         dataset_train_Label,\n",
    "                         cv=10, \n",
    "                         scoring='f1')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning of Random Forest\n",
    "\n",
    "We select the Random Forest model since it has the highest F1_score. We now perform the fine tuning using grid search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 2, 'max_leaf_nodes': 10, 'n_estimators': 100}\n",
      "0.468206689344\n",
      "Confusion matrix:\n",
      "[[867 119]\n",
      " [ 52 138]]\n"
     ]
    }
   ],
   "source": [
    "# Fine - tuning of the selected ML model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'n_estimators': [10, 30, 100], \n",
    "               'max_features': [2, 8, 20],  \n",
    "               'max_leaf_nodes': [5, 10, 20 ,40]}]\n",
    "\n",
    "forest_clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=5,scoring='f1')\n",
    "grid_search.fit(dataset_train_FeaturesX_encoded_normalized, dataset_train_Label)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_predict = best_model.predict(dataset_train_FeaturesX_encoded_normalized)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(dataset_train_Label,y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the test dataset\n",
    "\n",
    "We now test the tuned Random Forest model on the test dataset. First we need to transform the test dataset by scaling numerical features and converting categorical features into numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "dataset_test_FeaturesX_normalized = normalizer.transform(dataset_test_Features[num_attribs])\n",
    "\n",
    "# Encoding of categorical attributes and return numpy array\n",
    "dataset_test_FeaturesX_encoded = pd.get_dummies(dataset_test_Features[cat_attribs]).as_matrix()\n",
    "\n",
    "# Merge the two feature arrays\n",
    "dataset_test_FeaturesX_encoded_normalized = np.concatenate((dataset_test_FeaturesX_normalized, \n",
    "                                                            dataset_test_FeaturesX_encoded), axis=1)\n",
    "\n",
    "# Convert label from categorical to numerical\n",
    "dataset_test_Label.replace(['No','Yes'],[0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[202  45]\n",
      " [ 22  25]]\n",
      "F1 score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42735042735042733"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_predict = best_model.predict(dataset_test_FeaturesX_encoded_normalized)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(dataset_test_Label,y_test_predict))\n",
    "print('F1 score:')\n",
    "f1_score(dataset_test_Label, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
